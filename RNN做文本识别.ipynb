{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39714666",
   "metadata": {},
   "source": [
    "# 文本分类任务实战\n",
    "数据集构建：影评数据集进行情感分析(分类任务)\\\n",
    "词向量模型：加载训练好的词向量或者自己训练都可以\\\n",
    "序列网络模型：训练RNN模型进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36bd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import time\n",
    "import pprint\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322ccb2c",
   "metadata": {},
   "source": [
    "加载影评数据集，可以手动下载放到对应位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4599ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57db6d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000,), (25000,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd5e7c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 45,\n",
       " 24,\n",
       " 179,\n",
       " 4,\n",
       " 3679,\n",
       " 991,\n",
       " 25,\n",
       " 62,\n",
       " 440,\n",
       " 12,\n",
       " 62,\n",
       " 30,\n",
       " 448,\n",
       " 23,\n",
       " 4,\n",
       " 8247,\n",
       " 12,\n",
       " 9552,\n",
       " 21,\n",
       " 61155,\n",
       " 2927,\n",
       " 2538,\n",
       " 9,\n",
       " 131,\n",
       " 6,\n",
       " 12999,\n",
       " 4702,\n",
       " 18,\n",
       " 107,\n",
       " 185,\n",
       " 156,\n",
       " 43,\n",
       " 10789,\n",
       " 83,\n",
       " 650,\n",
       " 33,\n",
       " 4,\n",
       " 58,\n",
       " 526,\n",
       " 34,\n",
       " 308,\n",
       " 15706,\n",
       " 5,\n",
       " 398,\n",
       " 34,\n",
       " 20264,\n",
       " 5877,\n",
       " 4,\n",
       " 20,\n",
       " 186,\n",
       " 8,\n",
       " 30,\n",
       " 6,\n",
       " 2220,\n",
       " 7,\n",
       " 94,\n",
       " 58,\n",
       " 4,\n",
       " 522,\n",
       " 5581,\n",
       " 54,\n",
       " 298,\n",
       " 108,\n",
       " 71,\n",
       " 262,\n",
       " 18471,\n",
       " 21,\n",
       " 12,\n",
       " 131,\n",
       " 6041,\n",
       " 6,\n",
       " 3341,\n",
       " 88,\n",
       " 4,\n",
       " 65,\n",
       " 266,\n",
       " 180,\n",
       " 8,\n",
       " 1326,\n",
       " 7,\n",
       " 5293,\n",
       " 5,\n",
       " 9398,\n",
       " 15,\n",
       " 15107,\n",
       " 57,\n",
       " 551,\n",
       " 51,\n",
       " 810,\n",
       " 4,\n",
       " 598,\n",
       " 1360,\n",
       " 2398,\n",
       " 70,\n",
       " 131,\n",
       " 30,\n",
       " 421,\n",
       " 11,\n",
       " 4,\n",
       " 15048,\n",
       " 31454,\n",
       " 258,\n",
       " 11,\n",
       " 8389,\n",
       " 4711,\n",
       " 20060,\n",
       " 2527,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 6379,\n",
       " 114,\n",
       " 1160,\n",
       " 914,\n",
       " 2999,\n",
       " 6,\n",
       " 2340,\n",
       " 185,\n",
       " 13097,\n",
       " 37,\n",
       " 1068,\n",
       " 8,\n",
       " 847,\n",
       " 8,\n",
       " 3754,\n",
       " 8,\n",
       " 413,\n",
       " 6,\n",
       " 9962,\n",
       " 18,\n",
       " 3485,\n",
       " 18,\n",
       " 1026,\n",
       " 372,\n",
       " 368,\n",
       " 7,\n",
       " 1708,\n",
       " 21,\n",
       " 1892,\n",
       " 101,\n",
       " 11478,\n",
       " 29,\n",
       " 996,\n",
       " 3605,\n",
       " 21,\n",
       " 9,\n",
       " 8911,\n",
       " 8,\n",
       " 16301,\n",
       " 4108,\n",
       " 466,\n",
       " 27,\n",
       " 19511,\n",
       " 17040,\n",
       " 29,\n",
       " 892,\n",
       " 6,\n",
       " 3071,\n",
       " 10921,\n",
       " 4970,\n",
       " 3036,\n",
       " 773,\n",
       " 6042,\n",
       " 9203,\n",
       " 37,\n",
       " 86,\n",
       " 1085,\n",
       " 914,\n",
       " 17,\n",
       " 35,\n",
       " 776,\n",
       " 11847,\n",
       " 4,\n",
       " 107,\n",
       " 413,\n",
       " 11568,\n",
       " 23,\n",
       " 31,\n",
       " 160,\n",
       " 5,\n",
       " 9203,\n",
       " 778,\n",
       " 8,\n",
       " 1921,\n",
       " 914,\n",
       " 183,\n",
       " 216,\n",
       " 8,\n",
       " 6,\n",
       " 419,\n",
       " 33,\n",
       " 6,\n",
       " 7429,\n",
       " 1392,\n",
       " 10870,\n",
       " 1073,\n",
       " 121,\n",
       " 914,\n",
       " 417,\n",
       " 5273,\n",
       " 6,\n",
       " 2648,\n",
       " 8603,\n",
       " 2086,\n",
       " 9203,\n",
       " 461,\n",
       " 39392,\n",
       " 5,\n",
       " 4,\n",
       " 107,\n",
       " 270,\n",
       " 125,\n",
       " 18,\n",
       " 4202,\n",
       " 8,\n",
       " 2736,\n",
       " 6,\n",
       " 128,\n",
       " 113,\n",
       " 14,\n",
       " 9,\n",
       " 24,\n",
       " 6,\n",
       " 65,\n",
       " 15,\n",
       " 80,\n",
       " 1271,\n",
       " 8,\n",
       " 316,\n",
       " 11,\n",
       " 192,\n",
       " 49,\n",
       " 203,\n",
       " 131,\n",
       " 169,\n",
       " 12,\n",
       " 10579,\n",
       " 15,\n",
       " 6,\n",
       " 8975,\n",
       " 5,\n",
       " 6,\n",
       " 3036,\n",
       " 26,\n",
       " 679,\n",
       " 83,\n",
       " 2259,\n",
       " 2588,\n",
       " 246,\n",
       " 68,\n",
       " 20472,\n",
       " 235,\n",
       " 12812,\n",
       " 2902,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 27,\n",
       " 86,\n",
       " 678,\n",
       " 217,\n",
       " 2636,\n",
       " 4252,\n",
       " 9,\n",
       " 14140,\n",
       " 177,\n",
       " 17,\n",
       " 29,\n",
       " 961,\n",
       " 46,\n",
       " 11597,\n",
       " 2300,\n",
       " 1523,\n",
       " 13928,\n",
       " 5,\n",
       " 24920,\n",
       " 861,\n",
       " 13263,\n",
       " 17,\n",
       " 9203,\n",
       " 6458,\n",
       " 2858,\n",
       " 2976,\n",
       " 68600,\n",
       " 27,\n",
       " 2170,\n",
       " 1208,\n",
       " 1170,\n",
       " 1459,\n",
       " 39,\n",
       " 4,\n",
       " 7466,\n",
       " 5,\n",
       " 32090,\n",
       " 309,\n",
       " 11,\n",
       " 4,\n",
       " 965,\n",
       " 12687,\n",
       " 5,\n",
       " 56356,\n",
       " 440,\n",
       " 15,\n",
       " 511,\n",
       " 17,\n",
       " 35,\n",
       " 2990,\n",
       " 23175,\n",
       " 12702,\n",
       " 8,\n",
       " 914,\n",
       " 4,\n",
       " 4374,\n",
       " 7,\n",
       " 68,\n",
       " 5665,\n",
       " 9,\n",
       " 21288,\n",
       " 34,\n",
       " 32471,\n",
       " 22,\n",
       " 2199,\n",
       " 63,\n",
       " 6316,\n",
       " 2346,\n",
       " 4,\n",
       " 13447,\n",
       " 7,\n",
       " 4,\n",
       " 211,\n",
       " 4547,\n",
       " 7795,\n",
       " 33,\n",
       " 4,\n",
       " 58,\n",
       " 4,\n",
       " 167,\n",
       " 82,\n",
       " 2620,\n",
       " 17583,\n",
       " 9474,\n",
       " 7,\n",
       " 2183,\n",
       " 5,\n",
       " 939,\n",
       " 844,\n",
       " 8,\n",
       " 2233,\n",
       " 11,\n",
       " 4,\n",
       " 1730,\n",
       " 6461,\n",
       " 44543,\n",
       " 1109,\n",
       " 82,\n",
       " 738,\n",
       " 8,\n",
       " 140,\n",
       " 8,\n",
       " 5877,\n",
       " 18,\n",
       " 24,\n",
       " 3186,\n",
       " 4,\n",
       " 11463,\n",
       " 8821,\n",
       " 12666,\n",
       " 4,\n",
       " 6968,\n",
       " 7,\n",
       " 4,\n",
       " 65,\n",
       " 4,\n",
       " 85,\n",
       " 354,\n",
       " 26,\n",
       " 1533,\n",
       " 8171,\n",
       " 8,\n",
       " 4,\n",
       " 14069,\n",
       " 7,\n",
       " 4,\n",
       " 293,\n",
       " 105,\n",
       " 587,\n",
       " 4905,\n",
       " 19164,\n",
       " 17,\n",
       " 4,\n",
       " 255,\n",
       " 914,\n",
       " 892,\n",
       " 33,\n",
       " 4,\n",
       " 1073,\n",
       " 7752,\n",
       " 2064,\n",
       " 17,\n",
       " 6,\n",
       " 45256,\n",
       " 18917,\n",
       " 308,\n",
       " 47780,\n",
       " 17,\n",
       " 6,\n",
       " 1736,\n",
       " 27362,\n",
       " 5,\n",
       " 56888,\n",
       " 5701,\n",
       " 17,\n",
       " 6,\n",
       " 2610,\n",
       " 46,\n",
       " 7,\n",
       " 33505,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 107,\n",
       " 3815,\n",
       " 2945,\n",
       " 288,\n",
       " 5033,\n",
       " 1367,\n",
       " 6,\n",
       " 13397,\n",
       " 2492,\n",
       " 4473,\n",
       " 7,\n",
       " 4,\n",
       " 6114,\n",
       " 10219,\n",
       " 5,\n",
       " 7180,\n",
       " 1702,\n",
       " 39,\n",
       " 1325,\n",
       " 13969,\n",
       " 12803,\n",
       " 237,\n",
       " 472,\n",
       " 1082,\n",
       " 15706,\n",
       " 885,\n",
       " 5877,\n",
       " 26,\n",
       " 131,\n",
       " 581,\n",
       " 50,\n",
       " 26,\n",
       " 289,\n",
       " 1307,\n",
       " 20545,\n",
       " 23,\n",
       " 4,\n",
       " 333,\n",
       " 3815,\n",
       " 6,\n",
       " 168,\n",
       " 145,\n",
       " 664,\n",
       " 103,\n",
       " 2927,\n",
       " 11204,\n",
       " 23,\n",
       " 6,\n",
       " 356,\n",
       " 4480,\n",
       " 153,\n",
       " 303,\n",
       " 63,\n",
       " 944,\n",
       " 795,\n",
       " 39,\n",
       " 12803,\n",
       " 2858,\n",
       " 4252,\n",
       " 5,\n",
       " 409,\n",
       " 17,\n",
       " 73,\n",
       " 17,\n",
       " 2974,\n",
       " 5,\n",
       " 2472,\n",
       " 9360,\n",
       " 929,\n",
       " 141,\n",
       " 17,\n",
       " 21553,\n",
       " 268,\n",
       " 2181,\n",
       " 7096,\n",
       " 5,\n",
       " 8247,\n",
       " 63,\n",
       " 13027,\n",
       " 4,\n",
       " 14900,\n",
       " 7,\n",
       " 4,\n",
       " 1327,\n",
       " 2405,\n",
       " 28292,\n",
       " 675,\n",
       " 5,\n",
       " 1071,\n",
       " 3639,\n",
       " 8,\n",
       " 4,\n",
       " 22,\n",
       " 5,\n",
       " 6,\n",
       " 3350,\n",
       " 8,\n",
       " 4,\n",
       " 167,\n",
       " 9874,\n",
       " 15706]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[3453]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824a5f57",
   "metadata": {},
   "source": [
    "读进来的数据是已经转化成ID映射的，一般的数据读进来都是词语，需要手动转化成ID映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b4ce1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 14, 47, 8, 30, 31, 7, 4, 249, 108]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[2][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bed4e61",
   "metadata": {},
   "source": [
    "词和ID的映射表，空出来3个的目的是加上特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8df9a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "_word2idx = tf.keras.datasets.imdb.get_word_index()\n",
    "word2idx = {k: v + 3 for k, v in _word2idx.items()}\n",
    "word2idx['<pad>'] = 0\n",
    "word2idx['<start>'] = 1\n",
    "word2idx['<unk>'] = 2\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca64effd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fawn', 34704),\n",
       " ('tsukino', 52009),\n",
       " ('nunnery', 52010),\n",
       " ('sonja', 16819),\n",
       " ('vani', 63954),\n",
       " ('woods', 1411),\n",
       " ('spiders', 16118),\n",
       " ('hanging', 2348),\n",
       " ('woody', 2292),\n",
       " ('trawling', 52011)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word2idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc95d669",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(34704, 'fawn'),\n",
       " (52009, 'tsukino'),\n",
       " (52010, 'nunnery'),\n",
       " (16819, 'sonja'),\n",
       " (63954, 'vani'),\n",
       " (1411, 'woods'),\n",
       " (16118, 'spiders'),\n",
       " (2348, 'hanging'),\n",
       " (2292, 'woody'),\n",
       " (52011, 'trawling')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(idx2word.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c73b1",
   "metadata": {},
   "source": [
    "按文本长度大小进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31b97569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_len(x, y):\n",
    "    x, y = np.asarray(x), np.asarray(y)\n",
    "    idx = sorted(range(len(x)), key=lambda i: len(x[i]))\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85fe6a1e",
   "metadata": {},
   "source": [
    "将中间结果保存到本地，保存的是文本数据，方便调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a811ced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = sort_by_len(X_train, y_train)\n",
    "x_test, y_test = sort_by_len(X_train, y_test)\n",
    "\n",
    "def write_file(f_path, xs, ys):\n",
    "    with open(f_path, 'w', encoding='utf-8') as f:\n",
    "        for x, y in zip(xs, ys):\n",
    "            f.write(str(y) + '\\t' + ' '.join([idx2word[i] for i in x][1:]) + '\\n')\n",
    "\n",
    "write_file(\"./data/text/train.txt\", x_train, y_train)\n",
    "write_file(\"./data/text/test.txt\", x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b70765",
   "metadata": {},
   "source": [
    "# 构建语料表，基于词频来进行统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fa14f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 20598\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "with open('./data/text/train.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        label, words = line.split('\\t')\n",
    "        words = words.split(' ')\n",
    "        counter.update(words)\n",
    "\n",
    "words = ['<pad>'] + [w for w, freq in counter.most_common() if freq >= 10]\n",
    "print('Vocab Size:', len(words))\n",
    "\n",
    "Path(\"./vocab\").mkdir(exist_ok=True)\n",
    "\n",
    "with open('./vocab/word.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in words:\n",
    "        f.write(w + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe61cc0",
   "metadata": {},
   "source": [
    "得到新的word2idx映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "991527e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = dict()\n",
    "with open(\"./vocab/word.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for i, word in enumerate(f):\n",
    "        word = word.rstrip()\n",
    "        word2idx[word] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c185c5",
   "metadata": {},
   "source": [
    "# embedding层\n",
    "可以基于网络来训练，也可以加载别人训练好的，一般直接加载别人预训练好的模型\\\n",
    "常用预训练模型：https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8cca552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- At line 0\n",
      "- At line 100000\n",
      "- At line 200000\n",
      "- At line 300000\n"
     ]
    }
   ],
   "source": [
    "# 做了一个大表，里面有20598个不同的词，【20598*50】\n",
    "embedding = np.zeros((len(word2idx)+1, 50))   # + 1表示如果不在语料表中，就都是unknow\n",
    "\n",
    "with open('./data/glove.6B.50d.txt', encoding='utf-8') as f:    # 下载好的预训练模型\n",
    "    count = 0\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 100000 == 0:\n",
    "            print('- At line {}'.format(i))   # 打印处理了多少数据\n",
    "        line = line.rstrip()\n",
    "        sp = line.split(' ')\n",
    "        word, vec = sp[0], sp[1:]\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embedding[word2idx[word]] = np.asarray(vec, dtype='float32')  # 将词转换成对应的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ee5fe53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.18000013e-01,  2.49679998e-01, -4.12420005e-01,  1.21699996e-01,\n",
       "        3.45270008e-01, -4.44569997e-02, -4.96879995e-01, -1.78619996e-01,\n",
       "       -6.60229998e-04, -6.56599998e-01,  2.78430015e-01, -1.47670001e-01,\n",
       "       -5.56770027e-01,  1.46579996e-01, -9.50950012e-03,  1.16579998e-02,\n",
       "        1.02040000e-01, -1.27920002e-01, -8.44299972e-01, -1.21809997e-01,\n",
       "       -1.68009996e-02, -3.32789987e-01, -1.55200005e-01, -2.31309995e-01,\n",
       "       -1.91809997e-01, -1.88230002e+00, -7.67459989e-01,  9.90509987e-02,\n",
       "       -4.21249986e-01, -1.95260003e-01,  4.00710011e+00, -1.85939997e-01,\n",
       "       -5.22870004e-01, -3.16810012e-01,  5.92130003e-04,  7.44489999e-03,\n",
       "        1.77780002e-01, -1.58969998e-01,  1.20409997e-02, -5.42230010e-02,\n",
       "       -2.98709989e-01, -1.57490000e-01, -3.47579986e-01, -4.56370004e-02,\n",
       "       -4.42510009e-01,  1.87849998e-01,  2.78489990e-03, -1.84110001e-01,\n",
       "       -1.15139998e-01, -7.85809994e-01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f903ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19676 / 20598 words have found pre-trained values\n",
      "Saved ./vocab/word.py\n"
     ]
    }
   ],
   "source": [
    "print(\"%d / %d words have found pre-trained values\" % (count, len(word2idx)))\n",
    "np.save(\"./vocab/word.npy\", embedding)\n",
    "print('Saved ./vocab/word.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23bc9a28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20599, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ba7c0",
   "metadata": {},
   "source": [
    "# 构建训练数据\n",
    "注意所有的输入样本必须都是相同的shape(文本长度，词向量维度等)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43956bf5",
   "metadata": {},
   "source": [
    "# 数据生成器\n",
    "tf.data.Dataset.from_tensor_slices(tensor): 将tensor沿其第一个维度切片，返回一个含有N个样本的数据集，这样做的问题是需要将整个数据集整体传入，然后切片建立数据集类对象，比较占内存。\\\n",
    "tf.data.Dataset.from_generator(data_generator, output_data_type, output_data_shape): 从一个生成器中不断读取样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9df5c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(f_path, params):\n",
    "    with open(f_path, encoding='utf-8') as f:\n",
    "        print(\"Reading\", f_path)\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            label, text = line.split('\\t')\n",
    "            text = text.split(' ')\n",
    "            x = [params['word2idx'].get(w, len(word2idx)) for w in text]  # 得到当前词所对应的ID\n",
    "            if len(x) >= params['max_len']:  # 截断操作\n",
    "                x = x[:params['max_len']]\n",
    "            else:\n",
    "                x += [0] * (params['max_len'] - len(x))        # 补齐操作\n",
    "            y = int(label)\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb2008b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(is_training, params):\n",
    "    _shapes = ([params['max_len']], ())\n",
    "    _types = (tf.int32, tf.int32)\n",
    "    \n",
    "    if is_training:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: data_generator(params['train_path'], params),\n",
    "            output_shapes=_shapes,\n",
    "            output_types=_types\n",
    "        )\n",
    "        ds = ds.shuffle(params['num_samples'])\n",
    "        ds = ds.batch(params['batch_size'])\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)   # 设置缓存序列，根据可用的CPU动态设置并行调用的数量，说白了就是加速\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: data_generator(params['test_path'], params),\n",
    "            output_shapes=_shapes,\n",
    "            output_types=_types\n",
    "        )\n",
    "        ds = ds.batch(params['batch_size'])\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE) \n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963557c2",
   "metadata": {},
   "source": [
    "# 自定义网络模型\n",
    "定义好都有哪些层\\\n",
    "前向传播走一遍就行了"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f732a",
   "metadata": {},
   "source": [
    "BiLSTM(双向长短期记忆网络)\\\n",
    "相当于两层LSTM，batchsize*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d80c5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.Variable(np.load('./vocab/word.npy'),\n",
    "                                     dtype=tf.float32,\n",
    "                                     name='pretrained_embedding',\n",
    "                                     trainable=False\n",
    "                                    )\n",
    "        \n",
    "        self.drop1 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop2 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop3 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        \n",
    "        self.rnn1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "        self.rnn2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "        self.rnn3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=False))\n",
    "        \n",
    "        self.drop_fc = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.fc = tf.keras.layers.Dense(2*params['rnn_units'], tf.nn.elu)\n",
    "        \n",
    "        self.out_linear = tf.keras.layers.Dense(2)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        if inputs.dtype != tf.int32:\n",
    "            inputs = tf.cast(inputs, tf.int32)\n",
    "            \n",
    "        batch_sz = tf.shape(inputs)[0]\n",
    "        rnn_units = 2 * params['rnn_units']\n",
    "        \n",
    "        x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        \n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.rnn1(x)\n",
    "        \n",
    "        x = self.drop2(x, training=training)\n",
    "        x = self.rnn2(x)\n",
    "        \n",
    "        x = self.drop3(x, training=training)\n",
    "        x = self.rnn3(x)\n",
    "        \n",
    "        x = self.drop_fc(x, training=training)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.out_linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dbaee255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 速度更快\n",
    "class Model2(tf.keras.Model):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = tf.Variable(np.load('./vocab/word.npy'),\n",
    "                                     dtype=tf.float32,\n",
    "                                     name='pretrained_embedding',\n",
    "                                     trainable=False\n",
    "                                    )\n",
    "\n",
    "        self.drop1 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop2 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop3 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "\n",
    "        self.rnn1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "        self.rnn2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "        self.rnn3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "\n",
    "        self.drop_fc = tf.keras.layers.Dropout(params[\"dropout_rate\"])\n",
    "        self.fc = tf.keras.layers.Dense(2*params['rnn_units'], tf.nn.elu)\n",
    "\n",
    "        self.out_linear = tf.keras.layers.Dense(2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if inputs.type != tf.int32:\n",
    "            inputs = tf.cast(inputs, tf.int32)\n",
    "            \n",
    "        batch_sz = tf.shape(inputs)[0]\n",
    "        rnn_units = 2 * params['rnn_units']\n",
    "        \n",
    "        x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_sz*10*10, 10, 50))\n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.rnn1(x)\n",
    "        x = tf.reduce_max(x, 1)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_sz*10, 10, rnn_units))\n",
    "        x = self.drop2(x, training=training)\n",
    "        x = self.rnn2(x)\n",
    "        x = self.reduce_max(x, 1)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_sz, 10, rnn_units))\n",
    "        x = self.drop3(x, training=training)\n",
    "        x = self.rnn3(x)\n",
    "        x = tf.reduce_max(x, 1)\n",
    "        \n",
    "        x = self.drop_fc(x, training=training)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        x = self.out_linear(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74965a5e",
   "metadata": {},
   "source": [
    "# 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "827990eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'vocab_path': './vocab/word.txt',\n",
    "    'train_path': './data/text/train.txt',\n",
    "    'test_path': './data/text/test.txt',\n",
    "    'num_samples': 25000,\n",
    "    'num_labels': 2,\n",
    "    'batch_size': 32,\n",
    "    'max_len': 200,\n",
    "    'rnn_units': 128,\n",
    "    'dropout_rate': 0.2,\n",
    "    'clip_norm': 10,\n",
    "    'num_patience': 3,\n",
    "    'lr': 3e-4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d13e82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判断是否提前停止\n",
    "def is_descending(history: list):\n",
    "    history = history[-(params['num_patience']+1):]\n",
    "    for i in range(1, len(history)):\n",
    "        if history[i-1] <= history[i]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95711d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "with open(params['vocab_path'], encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.rstrip()\n",
    "        word2idx[line] = i\n",
    "params['word2idx'] = word2idx\n",
    "params['vocab_size'] = len(word2idx) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39361c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20598"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "304f9bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(params)\n",
    "model.build(input_shape=(None, None))   # 设置输入的大小， 或者fit时候也能自动找到\n",
    "\n",
    "decay_lr = tf.optimizers.schedules.ExponentialDecay(params['lr'], 1000, 0.95)    # 相当于加了一个指数衰减函数\n",
    "optim = tf.optimizers.Adam(params['lr'])\n",
    "global_step = 0\n",
    "\n",
    "history_acc = []\n",
    "best_acc = .0\n",
    "\n",
    "t0 = time.time()\n",
    "logger = logging.getLogger('tensorflow')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aceaa6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data/text/train.txt\n",
      "INFO:tensorflow:Step 0 | Loss: 0.6912 | Spent: 8.2 secs | LR: 0.000300\n",
      "INFO:tensorflow:Step 50 | Loss: 0.6728 | Spent: 268.8 secs | LR: 0.000299\n",
      "INFO:tensorflow:Step 100 | Loss: 0.6815 | Spent: 536.9 secs | LR: 0.000298\n",
      "INFO:tensorflow:Step 150 | Loss: 0.7035 | Spent: 889.9 secs | LR: 0.000298\n",
      "INFO:tensorflow:Step 200 | Loss: 0.5722 | Spent: 1323.2 secs | LR: 0.000297\n",
      "INFO:tensorflow:Step 250 | Loss: 0.4185 | Spent: 1694.4 secs | LR: 0.000296\n",
      "INFO:tensorflow:Step 300 | Loss: 0.5918 | Spent: 2101.0 secs | LR: 0.000295\n",
      "INFO:tensorflow:Step 350 | Loss: 0.6386 | Spent: 2485.0 secs | LR: 0.000295\n",
      "INFO:tensorflow:Step 400 | Loss: 0.5774 | Spent: 2849.7 secs | LR: 0.000294\n",
      "INFO:tensorflow:Step 450 | Loss: 0.5334 | Spent: 3206.8 secs | LR: 0.000293\n",
      "INFO:tensorflow:Step 500 | Loss: 0.5720 | Spent: 3567.1 secs | LR: 0.000292\n",
      "INFO:tensorflow:Step 550 | Loss: 0.5842 | Spent: 3929.6 secs | LR: 0.000292\n",
      "INFO:tensorflow:Step 600 | Loss: 0.6052 | Spent: 4309.4 secs | LR: 0.000291\n",
      "INFO:tensorflow:Step 650 | Loss: 0.4478 | Spent: 4685.5 secs | LR: 0.000290\n",
      "INFO:tensorflow:Step 700 | Loss: 0.5637 | Spent: 5051.3 secs | LR: 0.000289\n",
      "INFO:tensorflow:Step 750 | Loss: 0.4234 | Spent: 5410.1 secs | LR: 0.000289\n",
      "Reading ./data/text/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.501\n",
      "INFO:tensorflow:Best Accuracy: 0.501\n",
      "Reading ./data/text/train.txt\n",
      "INFO:tensorflow:Step 800 | Loss: 0.6070 | Spent: 8065.0 secs | LR: 0.000288\n",
      "INFO:tensorflow:Step 850 | Loss: 0.5214 | Spent: 8422.8 secs | LR: 0.000287\n",
      "INFO:tensorflow:Step 900 | Loss: 0.4677 | Spent: 8794.2 secs | LR: 0.000286\n",
      "INFO:tensorflow:Step 950 | Loss: 0.5726 | Spent: 9161.0 secs | LR: 0.000286\n",
      "INFO:tensorflow:Step 1000 | Loss: 0.6297 | Spent: 9512.0 secs | LR: 0.000285\n",
      "INFO:tensorflow:Step 1050 | Loss: 0.5612 | Spent: 9865.6 secs | LR: 0.000284\n",
      "INFO:tensorflow:Step 1100 | Loss: 0.6323 | Spent: 10211.3 secs | LR: 0.000284\n",
      "INFO:tensorflow:Step 1150 | Loss: 0.4572 | Spent: 10557.0 secs | LR: 0.000283\n",
      "INFO:tensorflow:Step 1200 | Loss: 0.4697 | Spent: 10901.4 secs | LR: 0.000282\n",
      "INFO:tensorflow:Step 1250 | Loss: 0.5206 | Spent: 11246.3 secs | LR: 0.000281\n",
      "INFO:tensorflow:Step 1300 | Loss: 0.4940 | Spent: 11591.1 secs | LR: 0.000281\n",
      "INFO:tensorflow:Step 1350 | Loss: 0.6158 | Spent: 11934.2 secs | LR: 0.000280\n",
      "INFO:tensorflow:Step 1400 | Loss: 0.5154 | Spent: 12277.6 secs | LR: 0.000279\n",
      "INFO:tensorflow:Step 1450 | Loss: 0.4166 | Spent: 12624.6 secs | LR: 0.000278\n",
      "INFO:tensorflow:Step 1500 | Loss: 0.3770 | Spent: 12972.1 secs | LR: 0.000278\n",
      "INFO:tensorflow:Step 1550 | Loss: 0.4665 | Spent: 13319.3 secs | LR: 0.000277\n",
      "Reading ./data/text/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.497\n",
      "INFO:tensorflow:Best Accuracy: 0.501\n",
      "Reading ./data/text/train.txt\n",
      "INFO:tensorflow:Step 1600 | Loss: 0.5587 | Spent: 15859.1 secs | LR: 0.000276\n",
      "INFO:tensorflow:Step 1650 | Loss: 0.4764 | Spent: 16204.5 secs | LR: 0.000276\n",
      "INFO:tensorflow:Step 1700 | Loss: 0.4723 | Spent: 16551.3 secs | LR: 0.000275\n",
      "INFO:tensorflow:Step 1750 | Loss: 0.4514 | Spent: 16896.1 secs | LR: 0.000274\n",
      "INFO:tensorflow:Step 1800 | Loss: 0.5026 | Spent: 17242.0 secs | LR: 0.000274\n",
      "INFO:tensorflow:Step 1850 | Loss: 0.5362 | Spent: 17588.5 secs | LR: 0.000273\n",
      "INFO:tensorflow:Step 1900 | Loss: 0.6006 | Spent: 17932.1 secs | LR: 0.000272\n",
      "INFO:tensorflow:Step 1950 | Loss: 0.6286 | Spent: 18276.8 secs | LR: 0.000271\n",
      "INFO:tensorflow:Step 2000 | Loss: 0.4714 | Spent: 18621.6 secs | LR: 0.000271\n",
      "INFO:tensorflow:Step 2050 | Loss: 0.5095 | Spent: 18965.0 secs | LR: 0.000270\n",
      "INFO:tensorflow:Step 2100 | Loss: 0.3681 | Spent: 19309.7 secs | LR: 0.000269\n",
      "INFO:tensorflow:Step 2150 | Loss: 0.4789 | Spent: 19654.0 secs | LR: 0.000269\n",
      "INFO:tensorflow:Step 2200 | Loss: 0.5405 | Spent: 19998.7 secs | LR: 0.000268\n",
      "INFO:tensorflow:Step 2250 | Loss: 0.5274 | Spent: 20345.3 secs | LR: 0.000267\n",
      "INFO:tensorflow:Step 2300 | Loss: 0.5109 | Spent: 20690.8 secs | LR: 0.000267\n",
      "Reading ./data/text/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.500\n",
      "INFO:tensorflow:Best Accuracy: 0.501\n",
      "Reading ./data/text/train.txt\n",
      "INFO:tensorflow:Step 2350 | Loss: 0.6844 | Spent: 23291.9 secs | LR: 0.000266\n",
      "INFO:tensorflow:Step 2400 | Loss: 0.5881 | Spent: 23657.8 secs | LR: 0.000265\n",
      "INFO:tensorflow:Step 2450 | Loss: 0.4939 | Spent: 24022.1 secs | LR: 0.000265\n",
      "INFO:tensorflow:Step 2500 | Loss: 0.2649 | Spent: 24385.6 secs | LR: 0.000264\n",
      "INFO:tensorflow:Step 2550 | Loss: 0.3148 | Spent: 24749.1 secs | LR: 0.000263\n",
      "INFO:tensorflow:Step 2600 | Loss: 0.5489 | Spent: 25113.1 secs | LR: 0.000263\n",
      "INFO:tensorflow:Step 2650 | Loss: 0.5992 | Spent: 25476.5 secs | LR: 0.000262\n",
      "INFO:tensorflow:Step 2700 | Loss: 0.4126 | Spent: 25843.1 secs | LR: 0.000261\n",
      "INFO:tensorflow:Step 2750 | Loss: 0.4587 | Spent: 26207.0 secs | LR: 0.000261\n",
      "INFO:tensorflow:Step 2800 | Loss: 0.4965 | Spent: 26572.1 secs | LR: 0.000260\n",
      "INFO:tensorflow:Step 2850 | Loss: 0.4480 | Spent: 26935.6 secs | LR: 0.000259\n",
      "INFO:tensorflow:Step 2900 | Loss: 0.4492 | Spent: 27301.7 secs | LR: 0.000259\n",
      "INFO:tensorflow:Step 2950 | Loss: 0.5083 | Spent: 27665.5 secs | LR: 0.000258\n",
      "INFO:tensorflow:Step 3000 | Loss: 0.3784 | Spent: 28030.7 secs | LR: 0.000257\n",
      "INFO:tensorflow:Step 3050 | Loss: 0.3632 | Spent: 28395.7 secs | LR: 0.000257\n",
      "INFO:tensorflow:Step 3100 | Loss: 0.4316 | Spent: 28760.0 secs | LR: 0.000256\n",
      "Reading ./data/text/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.501\n",
      "INFO:tensorflow:Best Accuracy: 0.501\n",
      "Reading ./data/text/train.txt\n",
      "INFO:tensorflow:Step 3150 | Loss: 0.5383 | Spent: 31442.4 secs | LR: 0.000255\n",
      "INFO:tensorflow:Step 3200 | Loss: 0.3852 | Spent: 31810.3 secs | LR: 0.000255\n",
      "INFO:tensorflow:Step 3250 | Loss: 0.5241 | Spent: 32179.4 secs | LR: 0.000254\n",
      "INFO:tensorflow:Step 3300 | Loss: 0.2724 | Spent: 32547.6 secs | LR: 0.000253\n",
      "INFO:tensorflow:Step 3350 | Loss: 0.6725 | Spent: 32917.6 secs | LR: 0.000253\n",
      "INFO:tensorflow:Step 3400 | Loss: 0.3756 | Spent: 33289.6 secs | LR: 0.000252\n",
      "INFO:tensorflow:Step 3450 | Loss: 0.4588 | Spent: 33660.9 secs | LR: 0.000251\n",
      "INFO:tensorflow:Step 3500 | Loss: 0.4801 | Spent: 34029.7 secs | LR: 0.000251\n",
      "INFO:tensorflow:Step 3550 | Loss: 0.2344 | Spent: 34399.9 secs | LR: 0.000250\n",
      "INFO:tensorflow:Step 3600 | Loss: 0.3332 | Spent: 34767.8 secs | LR: 0.000249\n",
      "INFO:tensorflow:Step 3650 | Loss: 0.2837 | Spent: 35136.7 secs | LR: 0.000249\n",
      "INFO:tensorflow:Step 3700 | Loss: 0.2918 | Spent: 35508.3 secs | LR: 0.000248\n",
      "INFO:tensorflow:Step 3750 | Loss: 0.3931 | Spent: 35878.5 secs | LR: 0.000248\n",
      "INFO:tensorflow:Step 3800 | Loss: 0.2760 | Spent: 36247.7 secs | LR: 0.000247\n",
      "INFO:tensorflow:Step 3850 | Loss: 0.4216 | Spent: 36616.8 secs | LR: 0.000246\n",
      "INFO:tensorflow:Step 3900 | Loss: 0.5578 | Spent: 36985.5 secs | LR: 0.000246\n",
      "Reading ./data/text/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.500\n",
      "INFO:tensorflow:Best Accuracy: 0.501\n",
      "Reading ./data/text/train.txt\n",
      "INFO:tensorflow:Step 3950 | Loss: 0.5477 | Spent: 39630.3 secs | LR: 0.000245\n",
      "INFO:tensorflow:Step 4000 | Loss: 0.5471 | Spent: 39997.0 secs | LR: 0.000244\n",
      "INFO:tensorflow:Step 4050 | Loss: 0.4479 | Spent: 40365.6 secs | LR: 0.000244\n",
      "INFO:tensorflow:Step 4100 | Loss: 0.3753 | Spent: 40732.2 secs | LR: 0.000243\n",
      "INFO:tensorflow:Step 4150 | Loss: 0.3855 | Spent: 41099.4 secs | LR: 0.000242\n",
      "INFO:tensorflow:Step 4200 | Loss: 0.4953 | Spent: 41465.6 secs | LR: 0.000242\n",
      "INFO:tensorflow:Step 4250 | Loss: 0.4899 | Spent: 41834.3 secs | LR: 0.000241\n",
      "INFO:tensorflow:Step 4300 | Loss: 0.4460 | Spent: 42198.5 secs | LR: 0.000241\n",
      "INFO:tensorflow:Step 4350 | Loss: 0.4194 | Spent: 42563.9 secs | LR: 0.000240\n",
      "INFO:tensorflow:Step 4400 | Loss: 0.3668 | Spent: 42929.1 secs | LR: 0.000239\n",
      "INFO:tensorflow:Step 4450 | Loss: 0.3358 | Spent: 43293.5 secs | LR: 0.000239\n",
      "INFO:tensorflow:Step 4500 | Loss: 0.3744 | Spent: 43661.2 secs | LR: 0.000238\n",
      "INFO:tensorflow:Step 4550 | Loss: 0.6806 | Spent: 44037.6 secs | LR: 0.000238\n",
      "INFO:tensorflow:Step 4600 | Loss: 0.5635 | Spent: 44428.1 secs | LR: 0.000237\n",
      "INFO:tensorflow:Step 4650 | Loss: 0.4274 | Spent: 44796.9 secs | LR: 0.000236\n",
      "Reading ./data/text/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.500\n",
      "INFO:tensorflow:Best Accuracy: 0.501\n",
      "Reading ./data/text/train.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step 4700 | Loss: 0.4614 | Spent: 47491.7 secs | LR: 0.000236\n",
      "INFO:tensorflow:Step 4750 | Loss: 0.4202 | Spent: 47902.7 secs | LR: 0.000235\n",
      "INFO:tensorflow:Step 4800 | Loss: 0.3594 | Spent: 48316.8 secs | LR: 0.000235\n",
      "INFO:tensorflow:Step 4850 | Loss: 0.3716 | Spent: 48730.1 secs | LR: 0.000234\n",
      "INFO:tensorflow:Step 4900 | Loss: 0.4767 | Spent: 49148.3 secs | LR: 0.000233\n",
      "INFO:tensorflow:Step 4950 | Loss: 0.4557 | Spent: 49566.3 secs | LR: 0.000233\n",
      "INFO:tensorflow:Step 5000 | Loss: 0.6009 | Spent: 49976.2 secs | LR: 0.000232\n",
      "INFO:tensorflow:Step 5050 | Loss: 0.3194 | Spent: 50392.9 secs | LR: 0.000232\n",
      "INFO:tensorflow:Step 5100 | Loss: 0.2405 | Spent: 50807.5 secs | LR: 0.000231\n",
      "INFO:tensorflow:Step 5150 | Loss: 0.3446 | Spent: 51222.8 secs | LR: 0.000230\n",
      "INFO:tensorflow:Step 5200 | Loss: 0.2864 | Spent: 51637.1 secs | LR: 0.000230\n",
      "INFO:tensorflow:Step 5250 | Loss: 0.3027 | Spent: 52053.1 secs | LR: 0.000229\n",
      "INFO:tensorflow:Step 5300 | Loss: 0.3739 | Spent: 52468.3 secs | LR: 0.000229\n",
      "INFO:tensorflow:Step 5350 | Loss: 0.5903 | Spent: 52881.5 secs | LR: 0.000228\n",
      "INFO:tensorflow:Step 5400 | Loss: 0.3799 | Spent: 53282.3 secs | LR: 0.000227\n",
      "INFO:tensorflow:Step 5450 | Loss: 0.4221 | Spent: 53696.2 secs | LR: 0.000227\n",
      "Reading ./data/text/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.499\n",
      "INFO:tensorflow:Best Accuracy: 0.501\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'fotmat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Accuracy: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(best_acc))\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(history_acc) \u001b[38;5;241m>\u001b[39m params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_patience\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m is_descending(history_acc):\n\u001b[1;32m---> 38\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTesting Accuracy not improved over \u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m epochs, Early Stop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfotmat\u001b[49m(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_patience\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'fotmat'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # 训练模型\n",
    "    for texts, labels in dataset(is_training=True, params=params):\n",
    "        with tf.GradientTape() as tape: # 梯度带，记录所有在上下文的操作，并且通过调用.gradient()获得任何上下文计算出的张量的梯度\n",
    "            logits = model(texts, training=True)\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        optim.lr.assign(decay_lr(global_step))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, params['clip_norm'])   # 将梯度限制一下，有时候更新太猛，防止过拟合\n",
    "        optim.apply_gradients(zip(grads, model.trainable_variables))    # 更新梯度\n",
    "        \n",
    "        if global_step % 50 == 0:\n",
    "            logger.info(\"Step {} | Loss: {:.4f} | Spent: {:.1f} secs | LR: {:6f}\".format(\n",
    "                global_step, loss.numpy().item(), time.time() - t0, optim.lr.numpy().item()\n",
    "            ))\n",
    "        \n",
    "        global_step += 1\n",
    "        \n",
    "    # 验证集效果\n",
    "    m = tf.keras.metrics.Accuracy()\n",
    "    \n",
    "    for texts, labels in dataset(is_training=False, params=params):\n",
    "        logits = model(texts, training=False)\n",
    "        y_pred = tf.argmax(logits, axis=1)\n",
    "        m.update_state(y_true=labels, y_pred=y_pred)\n",
    "    \n",
    "    acc = m.result().numpy()\n",
    "    logger.info('Evaluation: Testing Accuracy: {:.3f}'.format(acc))\n",
    "    history_acc.append(acc)\n",
    "    \n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    logger.info(\"Best Accuracy: {:.3f}\".format(best_acc))\n",
    "    \n",
    "    if len(history_acc) > params['num_patience'] and is_descending(history_acc):\n",
    "        logger.info(\"Testing Accuracy not improved over {} epochs, Early Stop\".format(params['num_patience']))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe3cd33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
